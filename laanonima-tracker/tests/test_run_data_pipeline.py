"""Tests for scrape retry/fallback behavior in run_data_pipeline script."""

import sys
import unittest
from datetime import datetime, timedelta, timezone
from pathlib import Path
from unittest.mock import patch

sys.path.insert(0, str(Path(__file__).parent.parent))

import scripts.run_data_pipeline as run_data_pipeline


class TestRunDataPipeline(unittest.TestCase):
    def _invoke(self, argv_extra, run_stage_impl, db_state=None, source_block=None):
        payload_holder = {}
        stage_calls = []

        def _fake_write_timing(_path, payload):
            payload_holder["payload"] = payload

        def _fake_run_stage(stages, name, command, env):
            stage_calls.append(name)
            return run_stage_impl(stages, name, command, env)

        config = {
            "deployment": {
                "public_base_url": "https://preciosushuaia.com",
                "output_dir": "public",
            },
            "website": {"base_url": "https://www.laanonima.com.ar/"},
            "branch": {"postal_code": "9410"},
        }
        default_db_state = db_state or {
            "prices_count": 5,
            "latest_scraped_at": datetime.now(timezone.utc).isoformat(),
        }
        default_source_block = source_block or (False, "ok")

        with patch.object(sys, "argv", ["run_data_pipeline.py", "--skip-smoke", *argv_extra]):
            with patch("scripts.run_data_pipeline.require_db_url", return_value="postgresql://u:p@h:5432/db"):
                with patch("scripts.run_data_pipeline.load_config", return_value=config):
                    with patch("scripts.run_data_pipeline.resolve_ipc_window", return_value=("2026-01", "2026-02")):
                        with patch("scripts.run_data_pipeline._run_stage", side_effect=_fake_run_stage):
                            with patch("scripts.run_data_pipeline._db_state", return_value=default_db_state):
                                with patch("scripts.run_data_pipeline._detect_source_block", return_value=default_source_block):
                                    with patch("scripts.run_data_pipeline.write_timing_payload", side_effect=_fake_write_timing):
                                        with patch("scripts.run_data_pipeline.write_github_summary"):
                                            with patch("scripts.run_data_pipeline.time.sleep"):
                                                code = run_data_pipeline.main()

        return code, payload_holder.get("payload", {}), stage_calls

    @staticmethod
    def _ok_run_stage(stages, name, command, _env):
        stages.append({"stage": name, "command": " ".join(command), "duration_seconds": 0.1})

    def test_scrape_ok_completes_without_warnings(self):
        code, payload, calls = self._invoke([], self._ok_run_stage)
        self.assertEqual(code, 0)
        self.assertEqual(payload.get("status"), "completed")
        self.assertEqual(payload.get("warnings"), [])
        self.assertFalse(payload.get("scrape_fallback_used"))
        self.assertIn("scrape-full-part-1", calls)
        self.assertIn("scrape-full-part-2", calls)
        self.assertEqual(payload.get("scrape_splits"), 2)

    def test_blocked_scrape_with_fresh_data_uses_fallback(self):
        attempts = {"scrape": 0}

        def _run_stage(stages, name, command, _env):
            if name.startswith("scrape-full"):
                attempts["scrape"] += 1
                raise RuntimeError("Failed to select branch: anti-bot challenge request could not be satisfied")
            stages.append({"stage": name, "command": " ".join(command), "duration_seconds": 0.1})

        fresh_state = {
            "prices_count": 8,
            "latest_scraped_at": (datetime.now(timezone.utc) - timedelta(hours=2)).isoformat(),
        }
        code, payload, _calls = self._invoke(
            ["--scrape-splits", "1"],
            _run_stage,
            db_state=fresh_state,
            source_block=(True, "marker=request could not be satisfied"),
        )
        self.assertEqual(code, 0)
        self.assertEqual(payload.get("status"), "completed")
        self.assertTrue(payload.get("scrape_fallback_used"))
        self.assertEqual(payload.get("scrape_block_retries_used"), 1)
        self.assertGreaterEqual(len(payload.get("warnings", [])), 1)
        self.assertEqual(attempts["scrape"], 2)

    def test_blocked_scrape_with_stale_data_fails(self):
        def _run_stage(stages, name, command, _env):
            if name.startswith("scrape-full"):
                raise RuntimeError("anti-bot challenge: generated by cloudfront")
            stages.append({"stage": name, "command": " ".join(command), "duration_seconds": 0.1})

        stale_state = {
            "prices_count": 8,
            "latest_scraped_at": (datetime.now(timezone.utc) - timedelta(days=8)).isoformat(),
        }
        code, payload, _calls = self._invoke(
            ["--scrape-splits", "1"],
            _run_stage,
            db_state=stale_state,
            source_block=(True, "marker=request could not be satisfied"),
        )
        self.assertEqual(code, 1)
        self.assertEqual(payload.get("status"), "failed")
        self.assertFalse(payload.get("scrape_fallback_used"))

    def test_blocked_scrape_with_empty_db_fails(self):
        def _run_stage(stages, name, command, _env):
            if name.startswith("scrape-full"):
                raise RuntimeError("anti-bot challenge: request could not be satisfied")
            stages.append({"stage": name, "command": " ".join(command), "duration_seconds": 0.1})

        empty_state = {
            "prices_count": 0,
            "latest_scraped_at": None,
        }
        code, payload, _calls = self._invoke(
            ["--scrape-splits", "1"],
            _run_stage,
            db_state=empty_state,
            source_block=(True, "marker=request could not be satisfied"),
        )
        self.assertEqual(code, 1)
        self.assertEqual(payload.get("status"), "failed")
        self.assertFalse(payload.get("scrape_fallback_used"))

    def test_non_blocking_scrape_error_fails_without_fallback(self):
        def _run_stage(stages, name, command, _env):
            if name.startswith("scrape-full"):
                raise RuntimeError("Postal input not found after trying all branch trigger selectors")
            stages.append({"stage": name, "command": " ".join(command), "duration_seconds": 0.1})

        code, payload, _calls = self._invoke(["--scrape-splits", "1"], _run_stage)
        self.assertEqual(code, 1)
        self.assertEqual(payload.get("status"), "failed")
        self.assertFalse(payload.get("scrape_fallback_used"))
        self.assertEqual(payload.get("warnings"), [])

    def test_two_splits_send_partition_flags(self):
        commands = []

        def _run_stage(stages, name, command, _env):
            if name.startswith("scrape-full-part-"):
                commands.append(command)
            stages.append({"stage": name, "command": " ".join(command), "duration_seconds": 0.1})

        code, payload, _calls = self._invoke(["--scrape-splits", "2"], _run_stage)
        self.assertEqual(code, 0)
        self.assertEqual(payload.get("scrape_splits"), 2)
        self.assertEqual(len(commands), 2)
        self.assertIn("--partition-count", commands[0])
        self.assertIn("--partition-index", commands[0])
        idx0 = commands[0].index("--partition-index")
        idx1 = commands[1].index("--partition-index")
        self.assertEqual(commands[0][idx0 + 1], "0")
        self.assertEqual(commands[1][idx1 + 1], "1")


if __name__ == "__main__":
    unittest.main()
