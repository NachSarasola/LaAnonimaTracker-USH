#!/usr/bin/env python3
"""Unified production data pipeline runner with timing telemetry."""

from __future__ import annotations

import argparse
import json
import re
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from time import perf_counter
from typing import Any, Dict, List
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen
import time

_SCRIPT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(_SCRIPT_ROOT))

from scripts.pipeline_common import (
    ROOT,
    build_env,
    cli_cmd,
    db_fingerprint,
    require_db_url,
    resolve_ipc_window,
    run_cmd,
    run_local_smoke,
    stage_record,
    write_github_summary,
    write_timing_payload,
)
from src.config_loader import load_config

_BLOCKING_SCRAPE_MARKERS = (
    "anti-bot challenge",
    "request could not be satisfied",
    "generated by cloudfront",
    "access denied",
    "captcha",
)


def _run_stage(stages: List[Dict[str, Any]], name: str, command: List[str], env: Dict[str, str]) -> None:
    duration = run_cmd(command, env)
    stages.append(stage_record(name=name, command=command, duration_seconds=duration))


def _db_state(env: Dict[str, str]) -> Dict[str, Any]:
    completed = subprocess.run(
        ["python", "scripts/check_db_state.py", "--backend", "postgresql", "--json"],
        cwd=ROOT,
        env=env,
        capture_output=True,
        text=True,
    )
    if completed.returncode != 0:
        return {}
    try:
        payload = json.loads((completed.stdout or "").strip() or "{}")
    except json.JSONDecodeError:
        return {}
    return payload if isinstance(payload, dict) else {}


def _parse_timestamp_utc(value: Any) -> datetime | None:
    if value is None:
        return None
    raw = str(value).strip()
    if not raw:
        return None
    normalized = raw.replace("Z", "+00:00")
    try:
        parsed = datetime.fromisoformat(normalized)
    except ValueError:
        return None
    if parsed.tzinfo is None:
        return parsed.replace(tzinfo=timezone.utc)
    return parsed.astimezone(timezone.utc)


def _compute_data_age_hours(latest_scraped_at: Any, reference_time_utc: datetime | None = None) -> float | None:
    reference_time_utc = reference_time_utc or datetime.now(timezone.utc)
    parsed = _parse_timestamp_utc(latest_scraped_at)
    if parsed is None:
        return None
    delta_seconds = max(0.0, (reference_time_utc - parsed).total_seconds())
    return round(delta_seconds / 3600.0, 3)


def _is_blocking_scrape_error(exc_text: str) -> bool:
    text = str(exc_text or "").strip().lower()
    if not text:
        return False
    return any(marker in text for marker in _BLOCKING_SCRAPE_MARKERS)


def _detect_source_block(config: Dict[str, Any]) -> tuple[bool, str]:
    website_cfg = config.get("website", {}) if isinstance(config.get("website"), dict) else {}
    branch_cfg = config.get("branch", {}) if isinstance(config.get("branch"), dict) else {}
    base_url = str(website_cfg.get("base_url", "https://www.laanonima.com.ar/")).strip().rstrip("/") + "/"
    postal_code = str(branch_cfg.get("postal_code", "9410")).strip()
    urls = [base_url, f"{base_url}?cp={postal_code}"]
    markers = [
        *_BLOCKING_SCRAPE_MARKERS,
        "the request could not be satisfied",
        "request could not be satisfied",
        "generated by cloudfront",
        "request blocked",
        "just a moment",
        "verify you are human",
        "checking your browser",
        "attention required",
        "cf-challenge",
        "challenge-platform",
        "access denied",
        "acceso denegado",
        "security check",
        "captcha",
    ]
    for url in urls:
        try:
            req = Request(url=url, headers={"User-Agent": "Mozilla/5.0"})
            with urlopen(req, timeout=12) as resp:
                body = resp.read(200_000).decode("utf-8", errors="ignore")
                status = int(getattr(resp, "status", 200) or 200)
                final_url = str(getattr(resp, "url", url) or url)
        except HTTPError as exc:
            body = ""
            try:
                body = exc.read().decode("utf-8", errors="ignore")
            except Exception:
                body = ""
            status = int(getattr(exc, "code", 0) or 0)
            final_url = str(getattr(exc, "url", url) or url)
        except URLError as exc:
            return False, f"source_unreachable:{exc}"
        except Exception as exc:
            return False, f"source_probe_error:{exc}"

        title_match = re.search(r"<title[^>]*>(.*?)</title>", body, flags=re.IGNORECASE | re.DOTALL)
        title = " ".join(title_match.group(1).split()) if title_match else ""
        combined = f"{title}\n{body}".lower()
        for marker in markers:
            if marker in combined:
                return True, f"marker={marker}|status={status}|url={final_url}"
    return False, "ok"


def main() -> int:
    parser = argparse.ArgumentParser(description="Run unified production pipeline (scrape + IPC + publish + smoke).")
    parser.add_argument("--config", default=None, help="Optional config.yaml path")
    parser.add_argument("--basket", default="all", help="Basket type for scrape/publish (default: all)")
    parser.add_argument("--view", default="analyst", help="Public report view (default: analyst)")
    parser.add_argument("--benchmark", default="ipc", help="Benchmark mode for report (default: ipc)")
    parser.add_argument(
        "--offline-assets",
        default="external",
        choices=["embed", "external"],
        help="Asset mode for public report (default: external)",
    )
    parser.add_argument("--skip-scrape", action="store_true", help="Skip scrape step")
    parser.add_argument("--skip-smoke", action="store_true", help="Skip local HTTP smoke test")
    parser.add_argument(
        "--require-empty",
        action="store_true",
        help="Require an empty production DB before continuing (bootstrap guard)",
    )
    parser.add_argument(
        "--ipc-lookback-months",
        type=int,
        default=2,
        help="Incremental IPC lookback window in months (default: 2)",
    )
    parser.add_argument("--ipc-from", default=None, help="Optional fixed IPC from month (YYYY-MM)")
    parser.add_argument("--ipc-to", default=None, help="Optional fixed IPC to month (YYYY-MM)")
    parser.add_argument(
        "--pdf-policy",
        choices=["always", "on_new_month", "never"],
        default="on_new_month",
        help="PDF validation policy for IPC sync (default: on_new_month)",
    )
    parser.add_argument(
        "--force-pdf-validation",
        action="store_true",
        help="Force PDF validation during IPC sync",
    )
    parser.add_argument("--commit-batch-size", type=int, default=None, help="Override scrape commit batch size")
    parser.add_argument("--base-request-delay-ms", type=int, default=None, help="Override scrape delay")
    parser.add_argument("--fail-fast-min-attempts", type=int, default=None, help="Override scrape fail-fast min attempts")
    parser.add_argument("--fail-fast-fail-ratio", type=float, default=None, help="Override scrape fail-fast ratio")
    parser.add_argument(
        "--scrape-block-retries",
        type=int,
        default=1,
        help="Retries for blocking scrape failures before fallback/fail (default: 1).",
    )
    parser.add_argument(
        "--scrape-block-retry-delay-seconds",
        type=int,
        default=45,
        help="Delay between blocking scrape retries in seconds (default: 45).",
    )
    parser.add_argument(
        "--max-data-staleness-days",
        type=int,
        default=7,
        help="Maximum data age (days) accepted for fallback/skip-scrape mode (default: 7).",
    )
    parser.add_argument(
        "--scrape-failure-mode",
        choices=["fail", "fallback_skip_if_blocked"],
        default="fallback_skip_if_blocked",
        help="Behavior when scrape stage fails (default: fallback_skip_if_blocked).",
    )
    parser.add_argument(
        "--timing-output",
        default="data/analysis/pipeline_timing_latest.json",
        help="Path for pipeline timing JSON",
    )
    args = parser.parse_args()

    env = build_env()
    stages: List[Dict[str, Any]] = []
    total_started = perf_counter()
    status = "completed"
    failure_message = ""
    failed_stage = ""
    warnings: List[str] = []
    scrape_fallback_used = False
    source_block_reason = ""
    data_age_hours: float | None = None
    scrape_block_retries_used = 0

    scrape_block_retries = max(0, int(args.scrape_block_retries))
    scrape_block_retry_delay_seconds = max(0, int(args.scrape_block_retry_delay_seconds))
    max_data_staleness_days = max(0, int(args.max_data_staleness_days))
    max_data_staleness_hours = float(max_data_staleness_days * 24)

    try:
        db_url = require_db_url()
    except Exception as exc:
        print(f"ERROR: {exc}", file=sys.stderr)
        return 2

    config = load_config(args.config)
    canonical_base = str(
        config.get("deployment", {}).get("public_base_url", "https://preciosushuaia.com")
    ).strip().rstrip("/")
    if not canonical_base:
        canonical_base = "https://preciosushuaia.com"

    try:
        ipc_from, ipc_to = resolve_ipc_window(
            ipc_from=args.ipc_from,
            ipc_to=args.ipc_to,
            lookback_months=args.ipc_lookback_months,
        )
    except Exception as exc:
        print(f"ERROR: {exc}", file=sys.stderr)
        return 2

    try:
        failed_stage = "init-db-schema"
        _run_stage(
            stages,
            name=failed_stage,
            command=["python", "scripts/check_db_state.py", "--backend", "postgresql", "--init-db"],
            env=env,
        )

        if args.require_empty:
            failed_stage = "require-empty-db"
            _run_stage(
                stages,
                name=failed_stage,
                command=["python", "scripts/check_db_state.py", "--backend", "postgresql", "--require-empty"],
                env=env,
            )

        failed_stage = "cli-init"
        _run_stage(stages, name=failed_stage, command=cli_cmd(args.config, "init"), env=env)

        if not args.skip_scrape:
            scrape_cmd = cli_cmd(
                args.config,
                "scrape",
                "--basket",
                args.basket,
                "--backend",
                "postgresql",
                "--profile",
                "full",
                "--candidate-storage",
                "db",
                "--observation-policy",
                "single+audit",
            )
            if args.commit_batch_size is not None:
                scrape_cmd.extend(["--commit-batch-size", str(args.commit_batch_size)])
            if args.base_request_delay_ms is not None:
                scrape_cmd.extend(["--base-request-delay-ms", str(args.base_request_delay_ms)])
            if args.fail_fast_min_attempts is not None:
                scrape_cmd.extend(["--fail-fast-min-attempts", str(args.fail_fast_min_attempts)])
            if args.fail_fast_fail_ratio is not None:
                scrape_cmd.extend(["--fail-fast-fail-ratio", str(args.fail_fast_fail_ratio)])

            failed_stage = "scrape-full"
            attempt_index = 0
            while True:
                attempt_index += 1
                scrape_started = perf_counter()
                try:
                    _run_stage(stages, name=failed_stage, command=scrape_cmd, env=env)
                    break
                except Exception as scrape_exc:
                    scrape_error_text = str(scrape_exc)
                    if not _is_blocking_scrape_error(scrape_error_text):
                        raise

                    blocked, block_probe_reason = _detect_source_block(config)
                    block_reason = block_probe_reason if blocked else f"blocking_error={scrape_error_text}"
                    source_block_reason = block_reason
                    stages.append(
                        {
                            "stage": failed_stage,
                            "command": " ".join(scrape_cmd),
                            "duration_seconds": round(perf_counter() - scrape_started, 3),
                            "result": "retryable_blocking_failure",
                            "attempt": attempt_index,
                            "error": scrape_error_text,
                            "reason": block_reason,
                        }
                    )

                    if attempt_index <= scrape_block_retries:
                        scrape_block_retries_used += 1
                        warning = (
                            f"scrape-full bloqueado (intento {attempt_index}/{scrape_block_retries + 1}); "
                            f"reintento en {scrape_block_retry_delay_seconds}s. reason={block_reason}"
                        )
                        warnings.append(warning)
                        print(f"WARN: {warning}")
                        if scrape_block_retry_delay_seconds > 0:
                            time.sleep(scrape_block_retry_delay_seconds)
                        continue

                    if args.scrape_failure_mode != "fallback_skip_if_blocked":
                        raise

                    state = _db_state(env)
                    prices_count = int(state.get("prices_count") or 0)
                    data_age_hours = _compute_data_age_hours(state.get("latest_scraped_at"))
                    has_fresh_data = data_age_hours is not None and data_age_hours <= max_data_staleness_hours
                    if prices_count > 0 and has_fresh_data:
                        scrape_fallback_used = True
                        warning = (
                            "scrape-full fallback activado: origen bloqueado, "
                            f"datos en DB={prices_count}, edad_horas={data_age_hours}, "
                            f"umbral_horas={max_data_staleness_hours}. reason={block_reason}"
                        )
                        warnings.append(warning)
                        print(f"WARN: {warning}")
                        stages.append(
                            {
                                "stage": failed_stage,
                                "command": " ".join(scrape_cmd),
                                "duration_seconds": round(perf_counter() - scrape_started, 3),
                                "result": "fallback_skip_if_blocked",
                                "attempt": attempt_index,
                                "error": scrape_error_text,
                                "reason": block_reason,
                                "prices_count": prices_count,
                                "data_age_hours": data_age_hours,
                                "max_data_staleness_hours": max_data_staleness_hours,
                            }
                        )
                        break

                    raise RuntimeError(
                        "Scrape bloqueado por anti-bot y fallback rechazado: "
                        f"prices_count={prices_count}, data_age_hours={data_age_hours}, "
                        f"max_allowed_hours={max_data_staleness_hours}, reason={block_reason}"
                    ) from scrape_exc

        failed_stage = "quality-gate-has-data"
        _run_stage(
            stages,
            name=failed_stage,
            command=[
                "python",
                "scripts/check_db_state.py",
                "--backend",
                "postgresql",
                "--require-has-data",
                "--require-fresh-max-age-hours",
                str(int(max_data_staleness_hours)),
            ],
            env=env,
        )

        ipc_sync_cmd = cli_cmd(
            args.config,
            "ipc-sync",
            "--region",
            "all",
            "--from",
            ipc_from,
            "--to",
            ipc_to,
            "--pdf-policy",
            args.pdf_policy,
        )
        if args.force_pdf_validation:
            ipc_sync_cmd.append("--force-pdf-validation")
        failed_stage = "ipc-sync"
        _run_stage(stages, name=failed_stage, command=ipc_sync_cmd, env=env)

        failed_stage = "ipc-build"
        _run_stage(
            stages,
            name=failed_stage,
            command=cli_cmd(args.config, "ipc-build", "--basket", args.basket, "--from", ipc_from, "--to", ipc_to),
            env=env,
        )

        failed_stage = "ipc-publish-patagonia"
        _run_stage(
            stages,
            name=failed_stage,
            command=cli_cmd(
                args.config,
                "ipc-publish",
                "--basket",
                args.basket,
                "--region",
                "patagonia",
                "--from",
                ipc_from,
                "--to",
                ipc_to,
                "--skip-sync",
                "--skip-build",
            ),
            env=env,
        )

        failed_stage = "ipc-publish-nacional"
        _run_stage(
            stages,
            name=failed_stage,
            command=cli_cmd(
                args.config,
                "ipc-publish",
                "--basket",
                args.basket,
                "--region",
                "nacional",
                "--from",
                ipc_from,
                "--to",
                ipc_to,
                "--skip-sync",
                "--skip-build",
            ),
            env=env,
        )

        failed_stage = "publish-web"
        _run_stage(
            stages,
            name=failed_stage,
            command=cli_cmd(
                args.config,
                "publish-web",
                "--basket",
                args.basket,
                "--view",
                args.view,
                "--benchmark",
                args.benchmark,
                "--offline-assets",
                args.offline_assets,
            ),
            env=env,
        )

        if not args.skip_smoke:
            failed_stage = "smoke-public-site"
            smoke_started = perf_counter()
            output_dir = Path(config.get("deployment", {}).get("output_dir", "public")).resolve()
            if not output_dir.exists():
                raise RuntimeError(f"Public output directory not found: {output_dir}")
            smoke_code = run_local_smoke(output_dir, expected_canonical_base=canonical_base)
            if smoke_code != 0:
                raise RuntimeError("Local smoke validation failed.")
            smoke_seconds = perf_counter() - smoke_started
            stages.append(stage_record(name=failed_stage, command=["local-smoke"], duration_seconds=smoke_seconds))

    except Exception as exc:
        status = "failed"
        failure_message = str(exc)
        print(f"\nERROR at stage '{failed_stage}': {exc}", file=sys.stderr)

    total_seconds = round(perf_counter() - total_started, 3)
    payload = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "status": status,
        "failed_stage": failed_stage if status == "failed" else "",
        "error": failure_message if status == "failed" else "",
        "warnings": warnings,
        "scrape_fallback_used": scrape_fallback_used,
        "source_block_reason": source_block_reason or None,
        "data_age_hours": data_age_hours,
        "scrape_block_retries_used": scrape_block_retries_used,
        "total_seconds": total_seconds,
        "db_fingerprint": db_fingerprint(db_url),
        "ipc_window": {"from": ipc_from, "to": ipc_to},
        "stages": stages,
    }
    timing_path = (ROOT / str(args.timing_output)).resolve()
    write_timing_payload(timing_path, payload)
    write_github_summary(payload)

    if status != "completed":
        return 1

    print("\nUnified production pipeline completed successfully.")
    return 0


if __name__ == "__main__":
    sys.exit(main())
